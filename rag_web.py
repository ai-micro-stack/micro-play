import os
from flask import Flask, request, render_template, jsonify
from plat.llmodel.llmodel_factory import LLModelFactory
from plat.vectordb.vectordb_factory import VectorDbFactory
from plat.embedding.embedding_factory import EmbeddingFactory
from rerank.rerank_retrieved_docs import get_context_from_documents
from dotenv import load_dotenv, set_key

load_dotenv()
ENV_PATH = ".env"
SUPPORTED_PROVIDERS = os.getenv("SUPPORTED_PROVIDERS")

EMBEDDING_PROVIDER = os.getenv("EMBEDDING_PROVIDER")
EMBEDDING_API_URL = os.getenv("EMBEDDING_API_URL")
EMBEDDING_API_KEY = os.getenv("EMBEDDING_API_KEY")

LLM_MODEL_PROVIDER = os.getenv("LLM_MODEL_PROVIDER")
LLM_MODEL_NAME = os.getenv("LLM_MODEL_NAME")
LLM_MODEL_API_URL = os.getenv("LLM_MODEL_API_URL")
LLM_MODEL_API_KEY = os.getenv("LLM_MODEL_API_KEY")

SUPPORTED_VECTORDBS = os.getenv("SUPPORTED_VECTORDBS")
VECTORDB_PROVIDER = os.getenv("VECTORDB_PROVIDER")
VECTORDB_TYPE = os.getenv("VECTORDB_TYPE")
VECTORDB_API_URL = os.getenv("LLM_MODEL_API_URL")
VECTORDB_API_KEY = os.getenv("LLM_MODEL_API_KEY")
VECTORDB_ROOT = ".vdb"

RETRIEVAL_DOCS = int(os.getenv("RETRIEVAL_DOCS"))
RELEVANT_DOCS = int(os.getenv("RELEVANT_DOCS"))

DEBUG = os.getenv("DEBUG")
DEBUG_MODE = DEBUG.lower() in ("true", "1", "yes", "on") if DEBUG else False


app = Flask(__name__)

# Initialize the retriever and LLM
llmodel_accessor = None
vectordb_accessor = None
embedding_accessor = None

# def get_vector_db_path(db_type, embedding_provider):
#     db_path = os.path.join(VECTORDB_ROOT, db_type + "-" + embedding_provider)
#     os.makedirs(db_path, exist_ok=True)
#     return db_path


def initialize_components():
    """Initialize the retriever and LLM components based on the current settings."""
    global llmodel_accessor, vectordb_accessor, embedding_accessor

    # choose the embedding model
    embedding_model = EmbeddingFactory(
        embedding_provider=EMBEDDING_PROVIDER, api_key=EMBEDDING_API_KEY
    )
    embedding_accessor = embedding_model.get_embedding_accessor()

    # choose the vectordb model
    vectordb_model = VectorDbFactory(
        vectordb_provider=VECTORDB_PROVIDER,
        db_type=VECTORDB_TYPE,
        api_key=VECTORDB_API_KEY,
    )
    vectordb_accessor = vectordb_model.get_vectordb_accessor()
    vectordb_accessor.set_embedding_function(embedding_accessor)

    # choose the llm model
    llm_model = LLModelFactory(
        llmodel_provider=LLM_MODEL_PROVIDER,
        model_name=LLM_MODEL_NAME,
        api_key=LLM_MODEL_API_KEY,
    )
    llmodel_accessor = llm_model.get_llmodel_accessor()

    print(
        f"Instantiating LLM from: {LLM_MODEL_PROVIDER} | LLM model: {LLM_MODEL_NAME} | Embed Provider: {EMBEDDING_PROVIDER} | VDB Provider: {VECTORDB_PROVIDER} | VDB Type: {VECTORDB_TYPE}"
    )


initialize_components()


@app.route("/")
def index():
    return render_template("index.html")


@app.route("/query", methods=["POST"])
def query():
    query_text = request.json["query_text"]

    # Retrieve and rerank the results
    results = vectordb_accessor.search_similar_chunks(query_text, RETRIEVAL_DOCS)
    enhanced_context_text, sources = get_context_from_documents(results, 3)

    # Generate response from LLM
    llm_response = llmodel_accessor.generate_response(
        context=enhanced_context_text, question=query_text
    )
    sources_html = "<br>".join(sources)
    response_text = f"{llm_response}<br><br>Reference Doc Snippet:<br>{sources_html}<br><br>(Response generated by {LLM_MODEL_PROVIDER} served LLM model: {LLM_MODEL_NAME})"
    return jsonify(response=response_text)


if __name__ == "__main__":
    app.run(debug=True)
